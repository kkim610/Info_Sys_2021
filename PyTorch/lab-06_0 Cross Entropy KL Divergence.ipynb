{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"lab-06_0 Cross Entropy KL Divergence.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"RqBy10wqRlDy"},"source":["## Information"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"h2zSaybzRlDz"},"source":["Information은 정보이론에서는 bit로 측정되며, 주어진 이벤트에서 발생하는 놀라움의 양 으로 이해할 수 있습니다. 엄청 불공평한 동전이 있다고 가정해봅시다. 앞면이 나올 확률이 99%, 뒷면이 나올 확률이 1%일때, 앞면이 나오는 일은 놀랍지 않습니다. 다만 뒷면이 나오면 굉장히 놀라겠죠. "]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"LDr1BQfhRlD0"},"source":["어떤 Event에 대한 Information의 수학적 정의는 다음과 같습니다.<br>\n","$$I(E)=-log(Pr(E))=-log(P)$$"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"VvfyS7SnRlD1"},"source":["특정한 stochastic event E에 대한 확률의 negative log로 나타낼 수 있으며, bits로 표현할 때는 밑이 2인 로그를 사용합니다. wmr,\n","$$I(E)=-log_2(Pr(E))=-log_2(P)$$\n","\n","**머신러닝에서는 종종 대안으로 자연로그($ln$)를 사용하며 bit 단위 대신 nat 단위로 표현합니다.(종종 단위를 생략합니다.)** <br>\n","\n","아무튼, 다시 동전 던지기로 넘어가서 <br>\n","앞면에 대한 information은 $-log_2(0.99) = 0.0144 bits$ 로 굉장히 낮으며, <br>\n","반대로 뒷면에 대한 information은 $-log_2(0.01) = 6.64 bits$ 로 높은 값을 갖습니다. <br>즉, 놀라움의 정도가 information에 잘 반영되어 있죠."]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"TKvre-oSRlD2"},"source":["## Entropy"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"F-8GSNZLRlD2"},"source":["기대값의 정의:\n","$$E[X]=\\sum_{i = 1}^{n} p(x_i)x_i $$"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"wW_KLPBaRlD3"},"source":["Entropy는 특정한 stochastic process에서 생성된 information의 평균! 즉, Information의 기대 값이다.\n","\n","$$H(x)=E[I(X)]=E[-log(P(X))]=-\\sum_{x = 1}^{n} p(x_i) log(p(x_i))$$"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"9IJkVcEnRlD3"},"source":["위의 동전 예시를 통해 Entropy H(X)를 구해보면 다음과 같습니다."]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"SYxpEacURlD4"},"source":["$$H(x)=-[0.99log_2(0.99)+0.01log_2(0.01)]=0.08bits$$"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"5BEnSmlZRlD4"},"source":["즉, 불공평한 동전은 0.08bits의 평균 정보 전달률을 갖는 stochastic information generator인 셈입니다. 공평한 동전(앞면 뒷면 각각 0.5)에 대해서 계산을 해보면 -(0.5 x -1 + 0.5 x -1) = 1bit가 나옵니다. 어떻게 보면, 불공평한 동전은 결과값을 예측하기 굉장히 쉬워서 Entropy 값이 낮게 나왔고, 공평한 동전은 결과값을 예측하는 게 굉장히 어렵기 때문에 Entropy 값이 높게 나온 셈이죠."]},{"cell_type":"markdown","metadata":{"id":"vc2skN0ERoSe"},"source":["강화학습에서 Entropy가 사용된다. 강화학습을 배우지 않았으므로 여기서는 더 이상 다루지 않기로 한다."]},{"cell_type":"markdown","metadata":{"id":"hO3_tkkESHoE"},"source":["## Cross entropy and KL divergence"]},{"cell_type":"markdown","metadata":{"id":"Yq6GRu46SLF1"},"source":[" Cross entropy는 두 확률 분포 P, Q 사이의 차이를 측정하는 지표입니다. 위에서 다룬 entropy는 하나의 확률 분포에 대한 측정 지표였다면, Cross entropy는 두 확률 분포에 대한 측정 지표인 셈이죠. 머신러닝에서 주로 사용되는 neural network에 대해 생각해보면, supervised learning 셋팅에서 Ground Truth가 존재하기 때문에 true probability distribution P가 존재하고, neural network가 학습을 통해 approximate probability distribution Q를 산출하게 됩니다. 이 때, P와 Q 사이의 거리 혹은 차이를 최소화할 필요가 있습니다. <br>\r\n"," 다음은 cross entropy 함수의 정의입니다.\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"ZAXPTEYxTHro"},"source":["$$H(P,Q)=H(P)+D_{KL}(P||Q)$$"]},{"cell_type":"markdown","metadata":{"id":"qW_7DA7xT6Xd"},"source":["$H(P)$는 true probability distribution $P$의 entropy를 의미하고, optimization 동안 고정되어 있고, optimization 과정에서 approximation probability distribution $Q$가 바뀌며 이에 따라  $D_{KL}(P||Q)$ 바뀝니다.<br>\r\n"," 이 두번째 term에서 분포 $P$와 분포 $Q$의 정보량의 차이가 정의됩니다. 즉, 두 확률 분포의 차이를 나타내는 지표인 cross entropy의 핵심은 두번째 term인 KL divergence입니다. 이제 KL divergence가 뭔 지 설명 드리겠습니다."]},{"cell_type":"markdown","metadata":{"id":"8hvpFbZ6gNEt"},"source":["## KL Divergence"]},{"cell_type":"markdown","metadata":{"id":"A5cYX667U1pY"},"source":["두 확률 분포 간의 KL divergence는 정보 이론적인 관점에서 보면 굉장히 다양한 해석이 가능하며, “놀라움”의 표현이기도 합니다. 두 확률 분포 P, Q가 가까웠다는 가정 하에, 만약 P와 Q가 가깝지 않다면 놀라운 일이며, 이 때 KL divergence는 높은 값을 갖게 되며, 반대로 가정대로 P와 Q가 가깝다면, 이는 놀랍지 않은 일이며 KL divergence도 낮은 값을 갖게 됩니다."]},{"cell_type":"markdown","metadata":{"id":"6WdSIOl7VQxW"},"source":["Bayesian 관점에서 보면 KL divergence는 prior distribution Q에서 posterior distribution P로 이동할 때 얻어지는 information을 의미합니다. KL divergence의 표현은 likelihood ratio approach를 통해 나타낼 수 있습니다. likelihood ratio는 아래와 같이 쉽게 표현이 가능합니다.\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"BA3LEa7XVQ_q"},"source":["$$LR=\\frac{p(x)}{q(x)}$$"]},{"cell_type":"markdown","metadata":{"id":"1R5c2hC_VRMa"},"source":["위 식은 다음과 같이 해석될 수 있다: 만약 어떠한 값 x가 임의의 분포로부터 sampling 되었을 때, likelihood ratio는 sample이 분포 $q$보다 분포 $p$에서 나왔을 확률을 의미합니다. $p$에서 나왔을 가능성이 높은 경우 LR은 1보다 큰 값을 갖고, 반대의 경우 1보다 작은 값을 갖습니다.\r\n"]},{"cell_type":"markdown","metadata":{"id":"EnWhUcgMWkCI"},"source":[" 독립적인 sample이 많이 있고, 이 모든 것들을 고려하여 likelihood function을 추정한다고 가정해봅시다. 그러면 아래와 같이 LR을 나타낼 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"vmIxa64MVRQp"},"source":["$$LR=\\prod_{i = 1}^{n}\\frac{p(x_i)}{q(x_i)}$$"]},{"cell_type":"markdown","metadata":{"id":"HHYW_LMaWq8i"},"source":["등호의 오른쪽 식의 ratio에 log를 씌워주면 곱셈이 합으로 바뀌게 된다. <br>\r\n"]},{"cell_type":"markdown","metadata":{"id":"xzAMVCFja7fL"},"source":["$$LR=\\sum_{i = 1}^{n}log\\left(\\frac{p(x_i)}{q(x_i)}\\right)$$"]},{"cell_type":"markdown","metadata":{"id":"U8QS9nx0a3zJ"},"source":["이를 log likelihood ratio라 부릅니다. 이제 우리는 likelihood ratio를 모종의 합으로 표현할 수 있게 되었습니다. <br>\r\n","이제 각 sample들이 평균적으로 $q(x)$보다 $p(x)$에서 얼마나 많이 나왔는지를 정량화 하는 문제에 대해 답해봅시다. 답하기 위해 우리는 likelihood ratio에 기대값을 취할 것입니다."]},{"cell_type":"markdown","metadata":{"id":"GxXztyHFWrAC"},"source":["$$D_{KL}(P||Q)=\\sum_{i = 1}^{n}p(x_i)log\\left(\\frac{p(x_i)}{q(x_i)}\\right)$$"]},{"cell_type":"markdown","metadata":{"id":"1Uq-mjQpWrDL"},"source":["이렇게 log likelihood ratio에 기대값을 취해준 값이 바로 KL divergence 입니다.<br> 즉, 정리하면 KL divergence는 얼마나 sampled data가 Q 분포 대신 P 분포로부터 나왔는지를 나타내는 likelihood ratio의 기대값입니다! 정리가 잘 되셨나요?"]},{"cell_type":"markdown","metadata":{"id":"pPAUB8oGdOvz"},"source":["위 식은 다음과 같이 다시 쓸 수 있다."]},{"cell_type":"markdown","metadata":{"id":"d9DX7GI7dOzg"},"source":["$$D_{KL}(P||Q)=\\sum_{i = 1}^{n}p(x_i)log(p(x_i)) - \\sum_{i = 1}^{n}p(x_i)log(q(x_i))$$"]},{"cell_type":"markdown","metadata":{"id":"lrUFzbqddO3L"},"source":["오른쪽 그림의 첫번째 term은 $P$ 분포에 대한 entropy를 의미합니다. 위의 entropy 설명 부분을 보시면 식이 정확히 일치하며, entropy는 information의 기대값을 의미 했었죠.<br>\r\n","식의 두번째 term은 뭔가 Q 분포에 대한 entropy를 나타내는 것 같지만, 자세히 들여다보면 $q(x)$가 아닌 $p(x)$가 곱해져 있습니다. 즉, $P$ 분포에 의해 weighted 되어서 계산이 됩니다."]},{"cell_type":"markdown","metadata":{"id":"j4oG1jGMdmdt"},"source":["이를 해석해보면, 만약 P가 true distribution인 경우에 KL divergence는 Q를 통해 표현할 때 손실된 정보의 양을 의미합니다.<br> 어렵죠? 사실 저도 잘 와 닿지 않습니다. 추상적으로 생각해보면, 만약 P 분포와 Q 분포가 거의 같은 분포였다면, P를 Q로 나타내도 정보의 손실이 거의 발생하지 않을 것입니다. 하지만 두 분포가 차이가 있었다면 P를 Q로 나타내는 과정에서 정보가 손실이 될 것이며, 이를 수식적으로 나타낸 값이 위의 식입니다."]},{"cell_type":"markdown","metadata":{"id":"lumd_eNvdmiD"},"source":["KL divergence의 가장 중요한 특징은 교환법칙이 성립하지 않는다는 점입니다."]},{"cell_type":"markdown","metadata":{"id":"QdUa1-z9dmkh"},"source":["$$D_{KL}(P||Q) \\neq D_{KL}(Q||P)$$"]},{"cell_type":"markdown","metadata":{"id":"MptKx-1xelnI"},"source":["즉, P와 Q의 KL divergence는 Q와 P의 KL divergence와 다른 값을 가집니다. 즉, KL divergence는 두 분포 간의 거리 개념이 아니며 distance metric도 아닙니다."]},{"cell_type":"markdown","metadata":{"id":"y-Sz2sxhelqu"},"source":["## Cross Entropy"]},{"cell_type":"markdown","metadata":{"id":"4F2RYjRjemHo"},"source":["위에서 정의했던 Cross entropy 함수를 다시 들여다보면, P와 Q의 Cross entropy는 true distribution P의 entropy와, P와 Q의 KL divergence의 합으로 정의가 되어있습니다. <br>\r\n","이 두 term을 더하면 Cross entropy를 아래와 같은 식으로 나타낼 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"5hU4owbcemLg"},"source":["\\begin{align}\r\n","H(P,Q) &= H(P)+D_{KL}(P||Q)\\\\\r\n","& = -\\sum_{x = 1}^{n} p(x_i) log(p(x_i))+\\sum_{i = 1}^{n}p(x_i)log(p(x_i)) - \\sum_{i = 1}^{n}p(x_i)log(q(x_i))\\\\\r\n","        & =- \\sum_{i = 1}^{n}p(x_i)log(q(x_i))\r\n","        \\end{align}"]},{"cell_type":"markdown","metadata":{"id":"Mnkg5noDg3mw"},"source":["이 식은 다들 친숙하실 것입니다. 주로 classification 문제를 풀 때 cross entropy loss를 사용하죠. 주로 true distribution $P$로는 one-hot encoded vector를 사용합니다. <br>\r\n","예를 들어 0~9까지 손으로 쓴 숫자를 분류하는 MNIST classification에서 숫자 2의 경우 true distribution $P$ = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] 이 됩니다."]},{"cell_type":"markdown","metadata":{"id":"HL6WYdRcg3po"},"source":["그리고 우리가 설계한 neural network의 output layer에 보통 classification의 경우 softmax layer를 마지막에 붙여줘서 output 값들이 0~1사이의 확률 값이 되고 다 더하면 1이 되도록 만들어줍니다. Output Layer의 결과 $Q$가 다음과 같다고 하자."]},{"cell_type":"markdown","metadata":{"id":"Bd4AvqDWg3tJ"},"source":["$$[0.01, 0.02, 0.75, 0.05, 0.02, 0.1, 0.001, 0.02, 0.009, 0.02]$$"]},{"cell_type":"markdown","metadata":{"id":"1O3VgcV_g3wZ"},"source":["neural network가 예측한 Q와 P의 차이를 측정하려면 어떻게 하면 좋을까요? 두 확률 분포 간의 차이를 측정하는 지표, 바로 Cross entropy를 사용하면 됩니다. P는 이미 one-hot encoding이 되어있기 때문에 i= 2를 제외한 $p(x_i)$=0 이 된다. 이렇게 P가 one-hot encoding 되어있는 경우 Cross entropy는 시그마를 사용하지 않고 나타낼 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"-azi_Zmkg30q"},"source":["$$H(P,Q)=-log(q(x_i))$$"]},{"cell_type":"markdown","metadata":{"id":"IYCSlm2Sy5yP"},"source":["위의 예시를 보면 Cross entropy loss는 $-ln(0.75) = 0.287$ 값을 갖게 됩니다. log 그래프를 떠올려보면, $q(x_2)$ 값이 1에 가까워질수록 loss는 감소하다가 0으로 가겠죠? 이런 식으로 classification 문제에서 Cross entropy loss가 사용이 되고 있습니다. 이렇게 Cross entropy를 최소화하면서 neural network를 학습시키게 되는데, 이 Cross entropy 식 자체가 P에 대한 Entropy와 P, Q간의 KL divergence의 합으로 구성이 되어있기 때문에 어떻게 보면 KL divergence를 최소화하는 것과 같습니다."]},{"cell_type":"markdown","metadata":{"id":"2JqTbKlIz5Vq"},"source":["One might wonder – if the cross entropy loss for classification tasks reduces to a single output node calculation, how does the neural network learn to both increase the softmax value that corresponds to the true index, and decrease the values of all the other nodes? It does this via the cross interaction of nodes through the weights, but also, through the nature of the softmax function itself – if a single index is encouraged to increase, all the other indices/output classes will be encouraged to decrease in the softmax function."]}]}